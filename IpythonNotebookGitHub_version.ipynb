{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing all the relevant imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np## Import Packages\n",
    "import cv2 #bringing in OpenCV libraries\n",
    "\n",
    "import math\n",
    "\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "\n",
    "import os\n",
    "\n",
    "from utils import color_selection,grayscale,gaussian_blur,canny,region_of_interest,hough_lines,draw_hough_lines_extrapolate,weighted_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the image\n",
    "image_original = mpimg.imread('test_images/solidWhiteCurve.jpg')\n",
    "# Note: always make a copy rather than simply using \"=\"\n",
    "image_original2 = np.copy(image_original)\n",
    "image_original3 = np.copy(image_original)\n",
    "\n",
    "image_copy = np.copy(image_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grayscale the image\n",
    "image_gray = grayscale(image_copy)\n",
    "mpimg.imsave('result_images/imageGreyScale.png', image_gray,cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image converted to gray scale.**\n",
    "\n",
    "---\n",
    "\n",    
    " <img src=\"https://raw.githubusercontent.com/dannofield/Self-driving-car/master/result_images/imageGreyScale.png\" width=\"380\" alt=\"Gray Scaled\" />\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Grey scaled image output </p> \n",
    " <p></p> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a kernel size and apply a Gaussian Noise kernel to smooth the image\n",
    "kernel_size = 3\n",
    "image_blur_gray = gaussian_blur(image_gray, kernel_size)\n",
    "\n",
    "# Define our parameters for Canny and apply the Canny transform\n",
    "low_threshold = 50 #35\n",
    "high_threshold = 200 #70\n",
    "image_edges_canny = canny(image_blur_gray, low_threshold, high_threshold)\n",
    "mpimg.imsave('result_images/imageCanny.png', image_edges_canny,cmap='Greys_r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After converting the image to gray scaled we need to remove noise from it. Noise can be removed by blurring it, and then the Canny Edge function was applied**\n",
    "\n",
    "---\n",
    "\n",
    " <img src=\"https://raw.githubusercontent.com/dannofield/Self-driving-car/master/result_images/imageCanny.png\" width=\"380\" alt=\"Canny Image\" />\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Canny image output </p> \n",
    " <p></p> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we'll create a masked edges image using cv2.fillPoly()\n",
    "# We are defining a four sided polygon to mask\n",
    "# Since we have video images of 960x540 and 1280x720 we are applying differente\n",
    "#shapes depending on the size of the image\n",
    "imshape = image_original.shape\n",
    "\n",
    "if(imshape[1] == 960): \t#small image uses this mask\n",
    "\tvertices = [(150,imshape[0]),(480, 300), (490, 300), (imshape[1]-30,imshape[0])]\n",
    "else:\t\t\t\t\t#large image uses this mask\n",
    "\tvertices = [(int(imshape[1]*6/32),660),(620, 430), (710, 430), (imshape[1]-150,650)]\n",
    "\n",
    "mask_polygon = np.array([vertices], dtype=np.int32)\n",
    "\n",
    "#This is just to show the zone we are masking\n",
    "cv2.line(image_original2, vertices[0],vertices[1], color=[0, 255, 0], thickness=2)\n",
    "cv2.line(image_original2, vertices[1],vertices[2], color=[0, 255, 0], thickness=2)\n",
    "cv2.line(image_original2, vertices[2],vertices[3], color=[0, 255, 0], thickness=2)\n",
    "\n",
    "\n",
    "mpimg.imsave('result_images/imageAreaOfInteres.png', image_original2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Region of Interest.\n",
    "The area inside this green triangle is the region of our interest.**\n",
    "\n",
    "---\n",
    "\n",
    " <img src=\"https://raw.githubusercontent.com/dannofield/Self-driving-car/master/result_images/imageAreaOfInteres.png\" width=\"380\" alt=\"Area of interes\" />\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Creating a mask for the area of interes </p> \n",
    " <p></p> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Hough transform parameters\n",
    "# Define the Hough transform parameters\n",
    "rho = 2                 #2,1 distance resolution in pixels of the Hough grid\n",
    "theta = np.pi/180       # angular resolution in radians of the Hough grid\n",
    "threshold = 80          #8,15,1 minimum number of votes (intersections in Hough grid cell) \n",
    "                        #meaning at least 15 points in image space need to be associated with each line segment\n",
    "min_line_length = 10    #40,10 minimum number of pixels making up a line\n",
    "max_line_gap = 13       #20,5 maximum gap in pixels between connectable line segments\n",
    "\n",
    "\n",
    "# Make a blank the same size as our image to draw on\n",
    "# Run Hough on edge detected image\n",
    "#It returns an image with lines drawn on it. It is a blank image (all black) with lines drawn on it\n",
    "image_hough_lines = hough_lines(image_edges_canny , rho, theta, threshold, min_line_length, max_line_gap)\n",
    "mpimg.imsave('result_images/imageHoughLinesUnmasked.png', image_hough_lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying Hough Transform to an unmasked image**\n",
    "\n",
    "---\n",
    "\n",
    " <img src=\"https://raw.githubusercontent.com/dannofield/Self-driving-car/master/result_images/imageHoughLinesUnmasked.png\" width=\"380\" alt=\"Hough Lines\" />\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Hough Lines using unmasked Canny's output</p> \n",
    " <p></p> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we are masking canny's output with out polygon\n",
    "image_masked_edges_canny = region_of_interest(image_edges_canny,mask_polygon)\n",
    "image_hough_lines_masked = hough_lines(image_masked_edges_canny , rho, theta, threshold, min_line_length, max_line_gap)\n",
    "\n",
    "#This is only necesary if you want the lines on Canny's image\n",
    "# Create a \"color\" binary image to combine with line image\n",
    "image_masked_edges_canny2 = np.dstack((image_masked_edges_canny, image_masked_edges_canny, image_masked_edges_canny)) \n",
    "\n",
    "image_combo_result_on_canny = weighted_img(image_hough_lines_masked ,image_masked_edges_canny2, alpha=0.8, beta=1., teta=0.)\n",
    "mpimg.imsave('result_images/imageHoughLinesPlusCanny.png', image_combo_result_on_canny)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drawing the image on canny's output after masking the image and using Hough Transform**\n",
    "\n",
    "---\n",
    "\n",
    " <img src=\"https://raw.githubusercontent.com/dannofield/Self-driving-car/master/result_images/imageHoughLinesPlusCanny.png\" width=\"380\" alt=\"Hough Lines Plus Canny' output masked\" />\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Hough Lines Plus Canny's output masked</p> \n",
    " <p></p> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the lines on the original image image_original\n",
    "image_combo_original= weighted_img(image_original,image_hough_lines_masked , alpha=0.8, beta=1., teta=0.)\n",
    "mpimg.imsave('result_images/imageHoughLinesPlusOriginal.png', image_combo_original)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Line Detection - Hough Transform over the initial image**\n",
    "\n",
    "---\n",
    "\n",
    " <img src=\"https://raw.githubusercontent.com/dannofield/Self-driving-car/master/result_images/imageHoughLinesPlusOriginal.png\" width=\"380\" alt=\"Hough Lines masked Plus Original\" />\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Hough Lines masked Plus Original</p> \n",
    " <p></p>\n",
    " \n",
    "## Improving the draw_lines() function\n",
    "\n",
    "**After we had the Hough line segments drawn onto the road, we can see that the Hough function returns a set of lines in the same direction. I draw them with random colors so we can see them all.**\n",
    "<p></p>\n",
    "\n",
    "    <div style=\"text-align: center;\">\n",
    " <img src=\"https://raw.githubusercontent.com/dannofield/Self-driving-car/master/result_images/imageHoughLinesPlusCannyLeftLines.png\" style='margin-top:0;width: 380px; display: inline-block;' alt=\"Hough Lines masked Left Line\"/>  <img src=\"https://raw.githubusercontent.com/dannofield/Self-driving-car/master/result_images/imageHoughLinesPlusCannyRightLine.png\" style='margin-top:0;width: 380px; display:inline-block;' alt=\"Hough Lines masked Right Line\" />\n",
    "        </div>\n",
    " <p><br></p> \n",
    " <p style=\"text-align: center;clear: both; \"> Individual left and right Hough Lines masked</p> \n",
    " <p></p>\n",
    " \n",
    "\n",
    "**I printed the information of every single line marking the main lines (right and left) on the road. I printed the starting point (x1,y1), the end point (x2,y2) and with these two points we can calculate the slope and the intersection of every single little line.**\n",
    "\n",
    "<p>\n",
    "<code>Left Lines\n",
    "x1:  280  y1:  461  x2:  320  y2:  430  m:  -0.775  b:  678.0\n",
    "x1:  437  y1:  344  x2:  467  y2:  320  m:  -0.8  b:  693.6\n",
    "x1:  293  y1:  462  x2:  353  y2:  412  m:  -0.8333333333333334  b:  706.167\n",
    "x1:  454  y1:  332  x2:  466  y2:  323  m:  -0.75  b:  672.5\n",
    "x1:  408  y1:  366  x2:  420  y2:  357  m:  -0.75  b:  672.0\n",
    "x1:  280  y1:  460  x2:  346  y2:  410  m:  -0.7575757575757576  b:  672.1212121\n",
    "\n",
    "Right Lines\n",
    "x1:  482  y1:  311  x2:  876  y2:  538  m:  0.5761421319796954  b:  33.29949238578678\n",
    "x1:  490  y1:  313  x2:  898  y2:  539  m:  0.553921568627451  b:  41.578431372549005\n",
    "x1:  711  y1:  434  x2:  787  y2:  477  m:  0.5657894736842105  b:  31.7236842105263\n",
    "x1:  650  y1:  407  x2:  726  y2:  450  m:  0.5657894736842105  b:  39.23684210526318\n",
    "</code></p>\n",
    "    \n",
    "**If we want to identify the full extent of the lane and marking it clearly as in the example video (P1_example.mp4), we can try to average and/or extrapolate the line segments we see on the images to map out the full extent of the lane lines.\n",
    "I identified each line as part of the left line or the right line of the road by the slope (negative or positive respectively).\n",
    "Then I created two arrays with these lines and I averaged them by using <code>np.average</code>.\n",
    "So we can end up with one single line that represents the set of lines returned by the Hough function.**\n",
    "\n",
    "<p><code>\n",
    "Average Left Lines\n",
    "[ -0.77765152 682.3979798 ]\n",
    "Average Right Lines\n",
    "[ 0.56541066 36.45961252]\n",
    "</code></p>\n",
    "\n",
    "**The new output draws a single, solid line over the left lane line and a single solid line over the right lane line. The lines start from the bottom of the image and extend out to the top of the region of interest (about 3/5th of the image).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_lines_extrapolate(img, lines, color=[255, 0, 0], thickness=8):\n",
    "\tif lines is not None:\n",
    "\t\tleft_line = []\n",
    "\t\tright_line = []\n",
    "\t\tfor line in lines:\n",
    "\t\t\tfor x1,y1,x2,y2 in line:\n",
    "\t\t\t\tm = (y2-y1)/(x2-x1)\t#get slope\n",
    "\t\t\t\tb = y1 - m*x1\t\t#get intercept\n",
    "\t\t\t\tif(m < 0. ):\n",
    "\t\t\t\t\tleft_line.append((m,b))\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tright_line.append((m,b))\n",
    "\n",
    "\t\t#Did we atleast have something?\n",
    "\t\tif left_line:\n",
    "\t\t\t#Get average of all the left lines found\n",
    "\t\t\tleft_avg_line = np.average(left_line,axis=0)\t\t\n",
    "\t\t\t#Calculate its coordinates from m & b\n",
    "\t\t\tavgslope,avgintercept = left_avg_line\t\t\t\n",
    "            #Do not use faulty data \n",
    "\t\t\tif not (avgslope == -np.inf or avgslope == np.inf or avgslope == 0):\n",
    "\t\t\t\ty1 = img.shape[0]\n",
    "\t\t\t\ty2 = int(y1*(3/5)) #only from the edge bottom up to 3/5th of the image\n",
    "\t\t\t\tx1 = int((y1-avgintercept)/avgslope)\n",
    "\t\t\t\tx2 = int((y2-avgintercept)/avgslope)\n",
    "\t\t\t\t#Draw left line on image\n",
    "\t\t\t\tcv2.line(img, (x1, y1), (x2, y2), color, thickness)\n",
    "\t\tif right_line:\n",
    "\t\t\t#Get average of all the right lines found\n",
    "\t\t\tright_avg_line = np.average(right_line,axis=0)\n",
    "\t\t\t#Calculate its coordinates from m & b\n",
    "\t\t\tavgslope,avgintercept = right_avg_line\n",
    "\t\t\tif not (avgslope == -np.inf or avgslope == np.inf or avgslope == 0):\n",
    "\t\t\t\ty1 = img.shape[0]\n",
    "\t\t\t\ty2 = int(y1*(3/5)) #only from the edge bottom up to 3/5th of the image\n",
    "\t\t\t\tx1 = int((y1-avgintercept)/avgslope)\n",
    "\t\t\t\tx2 = int((y2-avgintercept)/avgslope)\n",
    "\t\t\t\tcv2.line(img, (x1, y1), (x2, y2), color, thickness)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Image with single lines to each side**\n",
    " <img src=\"https://raw.githubusercontent.com/dannofield/Self-driving-car/master/result_images/imageHoughLinesExtrapolated.png\" width=\"380\" alt=\"Hough Line Extrapolated\" />\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Hough Line Extrapolated</p> \n",
    " <p></p> \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Challenge\n",
    "\n",
    "I tried to make it work. I figured out that the video had another resolution (1280x720). Also I saw that the part of the car that you can see on the bottom edge was affecting the algorithm.\n",
    "So I modified the region of interest to fit a bigger frame/image and to not include the bottom edge of the image.\n",
    "\n",
    "<code>\n",
    "#Since we have video images of 960x540 and 1280x720 we are applying differente\n",
    "#shapes depending on the size of the image\n",
    "\n",
    "if(imshape[1] == 960): \t#small image uses this mask\n",
    "\tvertices = [(150,imshape[0]),(480, 300), (490, 300), (imshape[1]-30,imshape[0])]\n",
    "else:\t\t\t\t\t#large image uses this mask\n",
    "\tvertices = [(int(imshape[1]*6/32),660),(620, 430), (710, 430), (imshape[1]-150,650)]\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "\n",
    "    <div style=\"text-align: center;\">\n",
    " <img src=\"https://raw.githubusercontent.com/dannofield/Self-driving-car/master/result_images/imageAreaOfInteres Challenge1.png\" style='margin-top:0;width: 380px; display: inline-block;' alt=\"Hough Lines masked Left Line\"/>  <img src=\"https://raw.githubusercontent.com/dannofield/Self-driving-car/master/result_images/imageHoughLinesExtrapolatedChallenge1.png\" style='margin-top:0;width: 380px; display:inline-block;' alt=\"Hough Lines masked Right Line\" />\n",
    "        </div>\n",
    " <p><br></p> \n",
    " <p style=\"text-align: center;clear: both; \"> Challenge video uses a different mask shape</p> \n",
    " <p></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when the road is whiter, the algorithm does not work at all. I guess it needs something more robust\n",
    " <p></p> \n",
    " <img src=\"https://raw.githubusercontent.com/dannofield/Self-driving-car/master/result_images/imageAreaOfInteresChallenge2.png\" width=\"380\" alt=\"Hough Line Extrapolated\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Whiter roads are not fun</p> \n",
    " <p></p> \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
},
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLAY & PROCESS VIDEOS IN REAL TIME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code will take all the videos from test_videos folder and will process them to show them in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing all the relevant imports\n",
    "#jupyter notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2 #bringing in OpenCV libraries\n",
    "\n",
    "import math\n",
    "\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "\n",
    "import os\n",
    "\n",
    "from utils import color_selection,grayscale,gaussian_blur,canny,region_of_interest,hough_lines,draw_hough_lines_extrapolate,weighted_img\n",
    "\n",
    "def process_image(image_original):\n",
    "\n",
    "\t# Note: always make a copy rather than simply using \"=\"\n",
    "\timage_copy = np.copy(image_original)\n",
    "\n",
    "\t# Read in and grayscale the image\n",
    "\timage_gray = grayscale(image_copy)\n",
    "\n",
    "\t# Define a kernel size and apply Gaussian smoothing\n",
    "\tkernel_size = 3\n",
    "\timage_blur_gray  = gaussian_blur(image_gray, kernel_size)\n",
    "\n",
    "\t# Define our parameters for Canny and apply the Canny transform\n",
    "\tlow_threshold = 50 #35\n",
    "\thigh_threshold = 200 #70\n",
    "\timage_edges_canny = canny(image_blur_gray, low_threshold, high_threshold)\n",
    "\n",
    "\t# Next we'll create a masked edges image using cv2.fillPoly()\n",
    "\t# We are defining a four sided polygon to mask\n",
    "\timshape = image_original.shape\n",
    "\t\n",
    "\tif(imshape[1] == 960): \t#small image use this mask\n",
    "\t\tvertices = [(150,imshape[0]),(480, 300), (490, 300), (imshape[1]-30,imshape[0])]\n",
    "\telse:\t\t\t\t\t#large image use this mask\n",
    "\t\tvertices = [(int(imshape[1]*6/32),660),(620, 430), (710, 430), (imshape[1]-150,650)]\n",
    "\n",
    "\tmask_polygon = np.array([vertices], dtype=np.int32)\n",
    "\timage_masked_edges_canny = region_of_interest(image_edges_canny,mask_polygon)\n",
    "\n",
    "\t# Define the Hough transform parameters\n",
    "\trho = 2\t\t\t\t#2,1 distance resolution in pixels of the Hough grid\n",
    "\ttheta = np.pi/180\t\t# angular resolution in radians of the Hough grid\n",
    "\tthreshold = 80\t\t\t#8,15,1 minimum number of votes (intersections in Hough grid cell) meaning at least 15 points in image space need to be associated with each line segment\n",
    "\tmin_line_length = 10\t\t#40,10 minimum number of pixels making up a line\n",
    "\tmax_line_gap = 13\t\t#20,5 maximum gap in pixels between connectable line segments\n",
    "\n",
    "\t# Make a blank the same size as our image to draw on\n",
    "\t# Run Hough on edge detected image\n",
    "\t#It returns an image with lines drawn on it, a blank image (all black) with lines drawn on it\n",
    "\timage_hough_lines_masked = draw_hough_lines_extrapolate(image_copy,image_masked_edges_canny , rho, theta, threshold, min_line_length, max_line_gap)\n",
    "\t#To generate videos you need to return this image\n",
    "\treturn image_hough_lines_masked\n",
    "\n",
    "\n",
    "\n",
    "videos = ['test_videos/solidWhiteRight.mp4','test_videos/solidYellowLeft.mp4','test_videos/challenge.mp4']\n",
    "indexVideo = 0\n",
    "video = cv2.VideoCapture(videos[indexVideo])\n",
    "\n",
    "######################################################\n",
    "#Create Videos Output uncomment this to create videos\n",
    "######################################################\n",
    "#white_output = 'test_videos_output/challenge.mp4'\n",
    "#clip1 = VideoFileClip(\"test_videos/challenge.mp4\")\n",
    "#white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "#white_clip.write_videofile(white_output, audio=False)\n",
    "\n",
    "#Play videos showing lines in Real Time\n",
    "while True:\n",
    "\tret,frame = video.read()\n",
    "\tif not ret:\n",
    "\t\t#Get another video\n",
    "\t\tindexVideo = indexVideo + 1\t\t\n",
    "\t\tvideo = cv2.VideoCapture(videos[indexVideo%3])\n",
    "\t\tcontinue\n",
    "\t#Process image and show the final image as a frame\n",
    "\tframeToShow = process_image(frame)\n",
    "\tcv2.imshow(\"frame\",frameToShow)\n",
    "\tkey = cv2.waitKey(25)\n",
    "\tif key == 27:\t#ESC Key to quit\n",
    "\t\tbreak;\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
